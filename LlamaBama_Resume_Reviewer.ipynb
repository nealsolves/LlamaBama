{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cf4a08",
   "metadata": {},
   "source": [
    "#### #Resume Scanner (Local only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e82b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import langchain\n",
    "import transformers\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "from torch import mps, cuda, bfloat16\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Guidance after occassional experiements -- last checked 11/24/2023\n",
    "\n",
    "imported_modules = ['os', 'shutil', 'torch', 'langchain', 'transformers', 'INSTRUCTOR', 'hub', 'CSVLoader', 'PDFMinerLoader', 'TextLoader', 'UnstructuredExcelLoader', 'Docx2txtLoader', 'UnstructuredFileLoader', 'UnstructuredMarkdownLoader', 'PyPDFLoader', 'UnstructuredPowerPointLoader', 'RecursiveCharacterTextSplitter', 'HuggingFaceInstructEmbeddings', 'HuggingFacePipeline', 'load_qa_chain', 'RetrievalQA', 'PromptTemplate', 'Chroma', 'Settings', 'AutoConfig', 'AutoModelForSeq2SeqLM', 'AutoModelForCausalLM', 'AutoTokenizer', 'GenerationConfig', 'pipeline', 'AutoModelForQuestionAnswering', 'mps', 'cuda', 'bfloat16', 'display', 'Markdown']\n",
    "\n",
    "all_names = dir()\n",
    "\n",
    "unused_modules = [module for module in imported_modules if module not in all_names]\n",
    "\n",
    "print(\"Unused modules/functions:\", unused_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d25c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "#Local Folder and Database\n",
    "ROOT_DIRECTORY = \"C:/Users/nilan/Documents/GenAI/GitHub/MyProjects/Doc-Reader\"\n",
    "# Document Folder\n",
    "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
    "# Database Folder\n",
    "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
    "\n",
    "# Output directory and File\n",
    "SUMMARY_DIRECTORY = f\"{ROOT_DIRECTORY}/SUMMARIES\"\n",
    "SUMMARY_DOCUMENT = \"summaries.txt\"\n",
    "\n",
    "# Local Models\n",
    "MODELS_DIRECTORY = \"C:/Users/nilan/Documents/GenAI/Tools/text-generation/text-generation-webui-310/models\"\n",
    "\n",
    "# Context Window and Max New Tokens\n",
    "# The context window of large language models (LLMs) refers to the range of tokens the model can consider when generating \n",
    "# responses to prompts. It determines how far back in the input sequence the model looks to understand context \n",
    "# and make predictions. LLMs with larger context windows can consider more preceding tokens, which can be beneficial \n",
    "# for tasks that require long-range dependencies or understanding complex contexts.\n",
    "CONTEXT_WINDOW_SIZE = 4096\n",
    "\n",
    "# MAX_NEW_TOKENS specifies the maximum number of tokens to generate in the output sequence, ignoring the number of tokens in \n",
    "# the input prompt. In other words, it determines the additional tokens beyond the prompt that the model generates. \n",
    "# For example, if you set max_new_tokens to 50, the model will generate up to 50 new tokens after the input prompt, \n",
    "# regardless of how long the prompt itself is.\n",
    "MAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n",
    "\n",
    "## In case of \"not enough space in the buffer\" error, reduce the values below. \n",
    "## Start with half of the original values and keep halving the value until the error stops appearing\n",
    "# n_gpu_layers option in llama.cpp allows you to specify the number of transformer layers that should be offloaded to the GPU \n",
    "# during inference. By doing so, you can accelerate the computation by leveraging the parallel processing capabilities of the GPU\n",
    "N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\n",
    "\n",
    "# n_batch determines the number of prompt tokens processed in parallel during inference. Don't mess around with it unless you face errors\n",
    "N_BATCH = 512\n",
    "\n",
    "# Threads refer to virtual slices of the workload your operating system maps to your CPU cores. The number of threads you configure \n",
    "# affects how efficiently your CPU cores are utilized during inference. Recommended value: your number of physical cores. Useless for GPU-only inference.\n",
    "THREADS = os.cpu_count()\n",
    "\n",
    "# Threads_batch is the number of threads allocated for batch processing. Optimizing the number of threads is crucial for performance. \n",
    "# The optimal thread count depends on your specific hardware configuration (number of CPU cores, hyperthreading, etc.).\n",
    "# Recommended value: total number of CPU cores (physical + virtual). Useless for GPU-only inference.\n",
    "THREADS_BATCH = os.cpu_count()*2\n",
    "\n",
    "# Temperature is a hyperparameter that regulates the randomness, or creativity, of the AIâ€™s responses.\n",
    "# A higher temperature value typically makes the output more diverse and creative but might also \n",
    "# increase its likelihood of straying from the context.\n",
    "\n",
    "TEMPERATURE = 0.01      # do_samples=True to enable Sample Decoding - Top-K / Top-P sampling etc.\n",
    "\n",
    "# Top-p, also known as nucleus sampling, controls the cumulative probability of the generated tokens. \n",
    "# The model generates tokens until the cumulative probability exceeds the chosen threshold (p). \n",
    "TOP_P = 0.9\n",
    "\n",
    "# Top-k provides a controlled randomness by considering a fixed number of top probable tokens.\n",
    "TOP_K = 50\n",
    "\n",
    "# Instructor Model\n",
    "# Add 2-5GB VRAM generally for embedding models.\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-xl\" # Uses 5 GB of VRAM (Great-ish accuracy)\n",
    "\n",
    "# Loaders \n",
    "DOCUMENT_MAP = {\n",
    "    \".txt\": TextLoader,\n",
    "    \".md\": UnstructuredMarkdownLoader,\n",
    "    \".py\": TextLoader,\n",
    "    \".pdf\": PyPDFLoader,\n",
    "    \".csv\": CSVLoader,\n",
    "    \".xls\": UnstructuredExcelLoader,\n",
    "    \".xlsx\": UnstructuredExcelLoader,\n",
    "    \".docx\": Docx2txtLoader,\n",
    "    \".doc\": Docx2txtLoader,\n",
    "    \".ppt\": UnstructuredPowerPointLoader,\n",
    "    \".pptx\": UnstructuredPowerPointLoader\n",
    "}\n",
    "\n",
    "# Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "device_type = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'mps' \n",
    "\n",
    "# Multi-Process Service (MPS) is a client-server implementation of the CUDA API for running multiple \n",
    "# processes concurrently on the same GPU. \n",
    "show_sources = True\n",
    "use_history = True\n",
    "save_qa = False\n",
    "\n",
    "# Model\n",
    "## (Full/Base MODELS)\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_basename = \"Llama-2-7b-chat-hf\"\n",
    "model_type = \"llama\"\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#model_basename = \"Mistral-7B-Instruct-v0.1\"\n",
    "#model_type= \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitarian Functions\n",
    "\n",
    "# Remove '\\n' character sequences from the documents\n",
    "def remove_newline_characters(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: remove_newline_characters(val) for key, val in data.items()}\n",
    "    elif isinstance(data, str):\n",
    "        return data.replace('\\n', '')\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_newline_characters(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Clear Chroma directory of old collections\n",
    "def clear_directory(directory):\n",
    "    # delete all files and folders in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Write Resume Summaries to a file in append mode\n",
    "def write_summary(question, result, summary_directory, summary_document):\n",
    "    # Define the summary file path\n",
    "    summary_file_path = os.path.join(summary_directory, summary_document).replace(\"\\\\\",\"/\")\n",
    "\n",
    "    # Open the file in append mode ('a')\n",
    "    with open(summary_file_path, 'a') as f:\n",
    "        # Write the result to the file\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef082c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Enter the Resume File Name with Extension in the cell output below:</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for the document to process\n",
    "\n",
    "while True:\n",
    "    # Ask user to input a file name\n",
    "    file_name = input(\"Please enter a file name: \")\n",
    "\n",
    "    # Validate the file name\n",
    "    if not os.path.splitext(file_name)[1]:\n",
    "        print(\"Error: File name must have an extension.\")\n",
    "    elif not os.path.exists(os.path.join(SOURCE_DIRECTORY, file_name)):\n",
    "        print(\"Error: File does not exist in the source directory. Please try again.\")\n",
    "    else:\n",
    "        print(f\"File {file_name} is valid and exists in the source directory.\")\n",
    "        break  # exit the loop if the file is valid and exists\n",
    "        \n",
    "# file_name = \"Duhamel, Jeff CV updated 21-Nov-2023 EN-GB.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c88f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# System Check\n",
    "\n",
    "print(f\"> CUDA Found: {cuda.is_available()}\\n\"\n",
    "      f\"> CUDA version: {torch.version.cuda}\\n\"\n",
    "      f\"> Chosen Model Type: {model_type}\\n\"\n",
    "      f\"> LLM will run on: {device_type}\\n\"\n",
    "      f\"> Display Source Documents set to: {show_sources}\\n\"\n",
    "      f\"> Use history set to: {use_history}\")\n",
    "\n",
    "# check if the persist_directory is empty before calling the function to clear it\n",
    "if not os.listdir(PERSIST_DIRECTORY):\n",
    "    print(f\"{PERSIST_DIRECTORY} is already empty\")\n",
    "else:\n",
    "    clear_directory(PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM paths for local inference\n",
    "\n",
    "model_dir = model_id.replace(\"/\",\"_\")\n",
    "model_path = os.path.join(MODELS_DIRECTORY, model_dir).replace(\"\\\\\",\"/\")\n",
    "cache_dir = f\"{model_path}/cache\"\n",
    "\n",
    "print(f\"> Local Model Directory: {model_dir}\\n> Local Model Path: {model_path}\\n> Local Cache Directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4aa940",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Skip the cell below if LLM has already been loaded and ipykernel is hot\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full Model (e.g., meta-llama/Llama-2-7b-chat-hf) \n",
    "\n",
    "\"\"\"\n",
    "text = \"<s>[INST] What is your favourite band? [/INST]\"\n",
    "\"Well, I'm quite partial to Mazzy Star. There are very few voices in music that strike you like the gentleness of Hope Sandoval. As the frontwoman of the long-revered and much-talked-about Mazzy Star, Sandoval mixed her tender falsettos with David Roback's jangly guitars to deliver heartfelt tunes that range from intimate folk to experimental psychedelia.</s> \"\n",
    "\"[INST] Tell me more about Mazzy Star and bands like Mazzy star in less than 100 words? [/INST]\"\n",
    "\"\"\"\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    use_flash_attention_2=False,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    config=model_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True, # set these if you are using NVIDIA GPU\n",
    "    #load_in_4bit=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_compute_dtype=torch.float16,\n",
    "    max_memory={0: \"16GB\"},  # for multi-GPU system {0: \"16GB\", 1: \"6GB\"} etc.  \n",
    "    local_files_only=True\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the LLM inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me more about Mazzy Star and bands like Mazzy star in less than 100 words?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to Mazzy Star. There are very few voices in music that strike you like the gentleness of Hope Sandoval. As the frontwoman of the long-revered and much-talked-about Mazzy Star, Sandoval mixed her tender falsettos with David Roback's jangly guitars to deliver heartfelt tunes that range from intimate folk to experimental psychedelia.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me more about Mazzy Star and bands like Mazzy star in less than 100 words?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device_type)\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\n> Loaded Local full model: {model}, tokenizer: {tokenizer}, on: {device_type}\")\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e018086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the question and answer from Llama 2 LLM inference above\n",
    "# ** may not work with other models that expect different format of prompts\n",
    "\n",
    "for item in decoded:\n",
    "    # Find start and end index of \"instruction\" part\n",
    "    inst_start = item.find(\"[INST]\") + 7\n",
    "    inst_end = item.find(\"[/INST]\")\n",
    "\n",
    "    # Extract instruction\n",
    "    instruction = item[inst_start:inst_end].strip()\n",
    "\n",
    "    # Extract completion\n",
    "    last_occurrence = max((i for i, char in enumerate(item) if char == ']'), default=None)\n",
    "    if last_occurrence is None:\n",
    "        raise ValueError(\"No matching ] found.\")\n",
    "    else:\n",
    "        completion = item[(last_occurrence + 3):].strip().replace(\"</s>\", \"\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n>Prompt:\\n{instruction}\\n\\n>Completion:\\n{completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0d25f",
   "metadata": {},
   "source": [
    "#### Load the Resume to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79802641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resume to process and split it in chunks\n",
    "\n",
    "try:\n",
    "    source_file_path = os.path.join(SOURCE_DIRECTORY, file_name).replace(\"\\\\\",\"/\")\n",
    "    file_extension = os.path.splitext(source_file_path)[1]\n",
    "    loader_class = DOCUMENT_MAP.get(file_extension)\n",
    "\n",
    "    if loader_class:\n",
    "        loader = loader_class(source_file_path)\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        documents = loader.load()\n",
    "        docs = doc_splitter.split_documents(documents)\n",
    "        docs = [remove_newline_characters(doc) for doc in docs]\n",
    "\n",
    "        print(f\"\\n> Loader selected: {loader_class}\"\n",
    "              f\"\\n> Loading {os.path.basename(source_file_path)} from {source_file_path}\"\n",
    "              f\"\\n> Pages parsed: {len(documents)}\"\n",
    "              f\"\\n> Split into {len(docs)} chunks of text\"\n",
    "              f\"\\n> First chunk: {docs[0]}\")\n",
    "    else:\n",
    "        print(f\"\\n> {source_file_path} document type is undefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n> Loading error: {str(e)} -> {source_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90768803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    embed_instruction=\"Represent the query for retrieval: \",\n",
    "    model_kwargs={\"device\": device_type},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "# change the embedding type if you are running into issues.\n",
    "# These are much smaller embeddings and will work for most appications\n",
    "# If you use HuggingFaceEmbeddings, make sure to also use the same during retrieval\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the documents to the database\n",
    "\n",
    "ids = [str(i) for i in range(1, len(docs) + 1)]\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings, ids=ids, client_settings=CHROMA_SETTINGS, persist_directory=PERSIST_DIRECTORY)\n",
    "\n",
    "print(f\"\\n> Collection Count: {db._collection.count()}\\n> {db.embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a text-gen Pipeline using transformers\n",
    "\n",
    "textgen_pipeline = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # model parameters\n",
    "    temperature=TEMPERATURE, #do_sample=True, \n",
    "    max_new_tokens=1024,  \n",
    "    repetition_penalty=1.15  # prevent output repetitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441741a",
   "metadata": {},
   "source": [
    "#### Optional Step for fun and some (in)sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pipeline for text generation using LLM\n",
    "\n",
    "res = textgen_pipeline(\n",
    "\"\"\"\n",
    "<s>[INST] Simulate three brilliant, logical experts collaboratively answering a question. \n",
    "Each one carefully thinks and verbosely explains their thought process in real-time, considering the \n",
    "prior explanations of others and openly acknowledging mistakes. At each step, whenever possible, \n",
    "each expert refines and builds upon the thoughts of others, acknowledging their contributions. \n",
    "They continue until there is a definitive answer to the question. After all experts have provided \n",
    "their analysis, you then analyze all 3 analyses and provide either the consensus solution or \n",
    "your best guess solution. Keep your response in less than 400 words.[/INST] The question is...\n",
    "\n",
    "Bob is in the living room.\n",
    "He walks to the kitchen, carrying a cup.\n",
    "He puts a ball in the cup and carries the cup to the bedroom.\n",
    "He turns the cup upside down, then walks to the garden.\n",
    "He puts the cup down in the garden, then walks to the garage.</s>\n",
    "[INST]Where is the ball?[/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(res[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680914fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please the Langchain Gods and wrap textgen pipeline with the LangChain HuggingFace wrapper. \n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=textgen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe1385",
   "metadata": {},
   "source": [
    "#### Llama Prompt for reference \n",
    "\n",
    "```python\n",
    "[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer: [/INST]\n",
    "    \n",
    "ChatPromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, template=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\", template_format='f-string', validate_template=True), additional_kwargs={})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up RetrievalQA Chain with Maximal Marginal Relevance (MMR) for text summarization\n",
    "# and for reducing redundancy and increasing diversity in the search results. \n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # try other chain types as well. refine, map_reduce, map_rerank\n",
    "    retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 3}), # MMR search will first fetch a pool of 3 or less documents, and then select the most diverse and relevant documents from this pool. \n",
    "    return_source_documents=True,  # verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247454b",
   "metadata": {},
   "source": [
    "#### Cells below are for Q&A. \n",
    "Just update the question or create a new cell below and copy pasta the cell content to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"The document is the resume of a Cloud FullStack developer named Jegan S. What is the name of the person in the document?\" \n",
    "output = qa_chain({\"query\": question})\n",
    "output[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are Jegan's strongest skills?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7299719",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Summarize Jegan's skills and experience in less than 200 words.\"\n",
    "output = qa_chain({\"query\": question})\n",
    "output[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f68e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Enter the question -- Intelligent Summarization -- worked\n",
    "\n",
    "question = \"\"\"\n",
    "Extract the following details about Jegan Sakthivel from the document:\n",
    "\n",
    "Name,\n",
    "Email,\n",
    "Phone Number,\n",
    "Key Skills,\n",
    "Certifications or Badges,\n",
    "Summary of Work Experience\n",
    "\n",
    "and provide these details in the following format in no more than 250 words\n",
    "\n",
    "Name:\n",
    "Email:\n",
    "Phone Number:\n",
    "Key Skills:\n",
    "Certifications or Badges:\n",
    "Summary of Work Experience:\n",
    "\"\"\"\n",
    "\n",
    "# Get the completion. Confirm it's not a hallucination by cross-checking with the resume.\n",
    "output = qa_chain({\"query\": question})\n",
    "print(\"\\n> \", output[\"result\"])\n",
    "\n",
    "\n",
    "##### DON'T CHANGE or DELETE the function call below #####\n",
    "\n",
    "# Use the write_summary function to write resume summaries every time a question is asked\n",
    "write_summary(question, output[\"result\"], SUMMARY_DIRECTORY, SUMMARY_DOCUMENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913c010",
   "metadata": {},
   "source": [
    "__Optional Step__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175082b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result to Markdown and display it\n",
    "display(Markdown(f\"**Result:**\\n\\n{result}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1cf3a",
   "metadata": {},
   "source": [
    "#### Extras below not needed right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral model for 128K context -- overkill for the job. not using\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Yarn-Mistral-7B-128k-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localGPT",
   "language": "python",
   "name": "localgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
